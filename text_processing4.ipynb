{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "analytics_document = nlp(\"He is reading books with other students while eating meal.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for token in analytics_document:\r\n",
    "    print(token.text, token.lemma_, token.is_stop)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "txt = \"This is the first sentence. However, all the other Ph.D. students are working on , say , second sentence.\"\r\n",
    "\r\n",
    "txt_analytics = nlp(txt)\r\n",
    "for sentence in txt_analytics.sents:\r\n",
    "    print(sentence, type(sentence))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is the first sentence. <class 'spacy.tokens.span.Span'>\n",
      "However, all the other Ph.D. students are working on , say , second sentence. <class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def nGrams(doc_string_or_list, n, docIsAlreadyTokenized):\r\n",
    "    doc = doc_string_or_list\r\n",
    "    if not docIsAlreadyTokenized:\r\n",
    "        doc = [token.text for token in nlp(doc_string_or_list)]\r\n",
    "    doc = \" \".join(doc).lower().split(' ')\r\n",
    "    grams = [doc[i:i+n] for i in range(len(doc)-n+1)]\r\n",
    "    return grams"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "doc = \"how old are you today or can you tell me something about you\"\r\n",
    "n = 2\r\n",
    "\r\n",
    "# n = 1 : 유니그램 (unigram)\r\n",
    "# n = 2 : 바이그램 (bigram)\r\n",
    "# n = 3 : 트라이그램 (trigram)\r\n",
    "# n >= 4 : n-gram\r\n",
    "\r\n",
    "grams = nGrams(doc, n, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "grams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['how', 'old'],\n",
       " ['old', 'are'],\n",
       " ['are', 'you'],\n",
       " ['you', 'today'],\n",
       " ['today', 'or'],\n",
       " ['or', 'can'],\n",
       " ['can', 'you'],\n",
       " ['you', 'tell'],\n",
       " ['tell', 'me'],\n",
       " ['me', 'something'],\n",
       " ['something', 'about'],\n",
       " ['about', 'you']]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "L = list(nlp.vocab.strings)\r\n",
    "numWords = len(L)\r\n",
    "print(numWords)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83814\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy  as np\r\n",
    "\r\n",
    "L = list(nlp.vocab.strings)\r\n",
    "numWords = len(L)\r\n",
    "\r\n",
    "W2I = dict(zip(L, np.arange(numWords)))\r\n",
    "I2W = dict(zip(np.arange(numWords), L))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(L)\r\n",
    "np.arange(numWords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "W2I[\"game\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49410"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "I2W[49410]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'game'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def oneHotVector(word, W2I, numWords):\r\n",
    "    v = np.zeros(numWords)\r\n",
    "    v[W2I[word]] = 1\r\n",
    "    return v"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "v = oneHotVector(\"game\", W2I, numWords)\r\n",
    "v, v.shape, v[49410], v[W2I[\"game\"]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.]), (83814,), 1.0, 1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "doc = \"How are you today. I know most of the time how you feel. .\"\r\n",
    "\r\n",
    "# 이 doc을 토큰화한 다음 리스트로 만들어 보세요\r\n",
    "tokens = [token.text for token in nlp(doc)]\r\n",
    "print(tokens)\r\n",
    "numWords = len(tokens)\r\n",
    "print(numWords)\r\n",
    "\r\n",
    "\r\n",
    "W2I = dict(zip(tokens, np.arange(numWords)))\r\n",
    "I2W = dict(zip(np.arange(numWords), tokens))\r\n",
    "print(W2I[\"feel\"])\r\n",
    "print(I2W[14])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['How', 'are', 'you', 'today', '.', 'I', 'know', 'most', 'of', 'the', 'time', 'how', 'you', 'feel', '.', '.']\n",
      "16\n",
      "13\n",
      ".\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "v = np.zeros(numWords)\r\n",
    "v"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "for token in tokens:\r\n",
    "    v += oneHotVector(token, W2I, numWords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "v"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}